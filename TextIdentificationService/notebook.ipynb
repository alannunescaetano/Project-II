{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR WITH KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imutils import build_montages\n",
    "\n",
    "from models import ResNet\n",
    "# from typing_extensions import Required\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "EPOCHS = 1\n",
    "INIT_LR = 1e-1\n",
    "BS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "A-Z\n",
    "https://www.nist.gov/srd/nist-special-database-19\n",
    "\n",
    "0-9\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load A-Z dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def load_az_dataset(dataset_path):\n",
    "    # initialize the list of data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the rows of the A-Z handwritten digit dataset\n",
    "    file = open(dataset_path, \"r\")\n",
    "    print(file)\n",
    "\n",
    "    for row in open(dataset_path, \"r\"):\n",
    "        # parse the label and image from the row\n",
    "        split_row = row.split(\",\")\n",
    "        label = int(split_row[0])\n",
    "        image = np.array([int(x) for x in split_row[1:]], dtype=\"uint8\")\n",
    "\n",
    "        # images are represented as single channel (grayscale) images\n",
    "        # that are 28x28=784 pixels -- we need to take this flattened\n",
    "        # 784-d list of numbers and reshape them into a 28x28 matrix\n",
    "        image = image.reshape((28, 28))\n",
    "\n",
    "        # update the list of data and labels\n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    # convert the data and labels to NumPy arrays\n",
    "    data = np.array(data, dtype=\"float32\")\n",
    "    labels = np.array(labels, dtype=\"int\")\n",
    "\n",
    "    # return a 2-tuple of the A-Z data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 0-9 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "def load_zero_nine_dataset():\n",
    "    # load the MNIST dataset and stack the training data and testing\n",
    "    # data together (we'll create our own training and testing splits\n",
    "    # later in the project)\n",
    "    ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n",
    "    data = np.vstack([trainData, testData])\n",
    "    labels = np.hstack([trainLabels, testLabels])\n",
    "    # return a 2-tuple of the MNIST data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:\\\\Projetos\\\\Mestrado\\\\Project II\\\\SourceCode\\\\TextIdentificationService\\\\datasets\\\\a_z\\\\A_Z Handwritten Data.csv' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "#ap = argparse.ArgumentParser()\n",
    "\n",
    "#ap.add_argument(\"-a\", \"--az\", required=True, help=\"path to A-Z dataset\")\n",
    "#ap.add_argument(\"-m\", \"--model\", type=str, required=True, help=\"path to output the trained handwriting recognition model\")\n",
    "#ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\", help=\"path to output the training history file\")\n",
    "\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# however, the architecture we're using is designed for 32x32 images,\n",
    "# so we need to resize them to 32x32\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "le = LabelBinarizer()\n",
    "\n",
    "labels = le.fit_transform(labels)\n",
    "ounts = labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training assesment balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max() / classTotals[i]\n",
    "\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "                                                  labels, test_size=0.20, stratify=None, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alanc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=10, zoom_range=0.05, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.15, horizontal_flip=False, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize and compile our deep neural network\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "                     (64, 64, 128, 256), reg=0.0005)\n",
    "                     \n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "  2328/297960 [..............................] - ETA: 123:33:14 - loss: 6.9873 - accuracy: 0.6155WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 297960 batches). You may need to use the repeat() function when building your dataset.\n",
      "297960/297960 [==============================] - 3829s 13ms/step - loss: 6.9873 - accuracy: 0.6155 - val_loss: 1.4746 - val_accuracy: 0.7929\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX), epochs=EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.70      0.76      0.73      2806\n",
      "           B       0.72      0.87      0.79      1673\n",
      "           C       0.91      0.82      0.86      4742\n",
      "           D       0.60      0.68      0.64      2044\n",
      "           E       0.90      0.93      0.91      2214\n",
      "           F       0.79      1.00      0.88       232\n",
      "           G       0.68      0.72      0.70      1183\n",
      "           H       0.76      0.63      0.69      1456\n",
      "           I       0.59      0.98      0.73       242\n",
      "           J       0.76      0.52      0.62      1655\n",
      "           K       0.67      0.45      0.54      1113\n",
      "           L       0.72      0.94      0.81      2306\n",
      "           M       0.79      0.74      0.76      2502\n",
      "           N       0.66      0.48      0.56      3700\n",
      "           O       0.87      0.85      0.86     11675\n",
      "           P       0.93      0.84      0.88      3840\n",
      "           Q       0.52      0.74      0.61      1184\n",
      "           R       0.77      0.59      0.67      2306\n",
      "           S       0.96      0.87      0.91      9663\n",
      "           T       0.98      0.97      0.97      4498\n",
      "           U       0.75      0.79      0.77      5774\n",
      "           V       0.48      0.95      0.64       863\n",
      "           W       0.46      0.78      0.58      2177\n",
      "           X       0.68      0.54      0.60      1259\n",
      "           Y       0.83      0.82      0.82      2172\n",
      "           Z       0.79      0.90      0.84      1212\n",
      "\n",
      "    accuracy                           0.79     74491\n",
      "   macro avg       0.74      0.77      0.75     74491\n",
      "weighted avg       0.81      0.79      0.79     74491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the list of label names\n",
    "#labelNames = \"0123456789\"\n",
    "labelNames = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), target_names=labelNames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model and training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model_path = r\"C:\\Projetos\\Mestrado\\Project II\\SourceCode\\TextIdentificationService\\model\\trained_ocr_only_az.model\"\n",
    "model.save(model_path, save_format=\"h5\")\n",
    "\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Trainning Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [] \n",
    "# randomly select a few testing characters\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
    "    # classify the character\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = labelNames[prediction[0]]\n",
    "\n",
    "    # extract the image from the test data and initialize the text\n",
    "    # label color as green (correct)\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "\n",
    "    # otherwise, the class label prediction is incorrect\n",
    "    if prediction[0] != np.argmax(testY[i]):\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "    # merge the channels into one image, resize the image from 32x32\n",
    "    # to 96x96 so we can better see it and then draw the predicted\n",
    "    # label on the image\n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "    color, 2)\n",
    "\n",
    "    # add the image to our list of output images\n",
    "    images.append(image)\n",
    "\n",
    "# construct the montage for the images\n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "\n",
    "# show the output montage\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading handwriting OCR model...\n",
      "C:\\Projetos\\Mestrado\\Project II\\SourceCode\\TextIdentificationService\\model\\trained_ocr_only_az.model\n",
      "[(82, 163, 30, 44), (118, 164, 25, 42), (149, 164, 36, 43), (160, 227, 41, 42), (204, 226, 41, 53), (204, 164, 33, 42), (216, 236, 17, 24), (241, 164, 26, 42), (251, 227, 36, 43), (270, 166, 40, 40), (270, 164, 39, 40), (295, 227, 12, 42), (311, 164, 33, 42), (347, 163, 41, 44)]\n",
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[[1.48674641e-02 2.99488995e-02 5.11613078e-02 8.22742563e-03\n",
      "  4.14830483e-02 8.10445379e-03 2.60123387e-02 3.14495922e-03\n",
      "  1.71966804e-03 9.26930341e-04 6.31180266e-03 4.07809298e-03\n",
      "  9.87663982e-04 4.76874004e-04 5.09871542e-01 7.04406854e-03\n",
      "  6.13670796e-03 8.87161568e-02 1.75797433e-01 3.75003001e-04\n",
      "  4.52358508e-03 4.96628694e-04 1.10663503e-04 1.10060058e-03\n",
      "  6.50089569e-05 8.31161626e-03]\n",
      " [1.13107143e-02 4.28760387e-02 4.11053747e-02 2.04864377e-03\n",
      "  2.62767881e-01 3.26097049e-02 6.58432320e-02 2.47847848e-03\n",
      "  1.03292789e-03 2.37254397e-04 9.03792400e-03 3.39420117e-03\n",
      "  5.28075325e-04 2.53866980e-04 1.32885888e-01 1.11914091e-02\n",
      "  2.68822256e-03 2.30483547e-01 1.36777148e-01 3.01296299e-04\n",
      "  1.84469135e-03 3.01448628e-04 5.00640381e-05 5.08919766e-04\n",
      "  2.18387813e-05 7.42112938e-03]\n",
      " [2.23189797e-02 5.32601669e-04 3.02419299e-03 5.97042264e-03\n",
      "  1.74077344e-04 1.40506192e-03 2.56345578e-04 1.09803081e-02\n",
      "  1.01581123e-03 1.21010351e-03 2.52241385e-03 1.10237417e-03\n",
      "  5.43208653e-03 3.76545712e-02 4.50082928e-01 8.81815155e-04\n",
      "  9.27037152e-04 3.19069922e-02 1.25062617e-03 4.30413114e-04\n",
      "  4.32227850e-02 3.71105671e-01 1.05924590e-03 4.47525084e-03\n",
      "  9.65812476e-04 9.20469538e-05]\n",
      " [5.67723885e-02 4.49663261e-03 3.04990052e-03 9.15288273e-03\n",
      "  6.02645683e-04 5.75478072e-04 7.58817245e-04 7.63257220e-03\n",
      "  7.70084327e-04 1.81562849e-04 3.68609210e-03 4.26577317e-04\n",
      "  1.19965209e-03 6.46157935e-03 8.20196271e-01 7.00490316e-04\n",
      "  3.19266715e-03 4.37792093e-02 4.96524572e-03 2.99435469e-05\n",
      "  2.16983650e-02 6.06272137e-03 7.23252364e-04 2.66427966e-03\n",
      "  3.61057937e-05 1.84597826e-04]\n",
      " [3.29357684e-02 4.69783368e-03 1.27569875e-02 1.96080748e-02\n",
      "  1.27528398e-03 2.40469910e-03 2.35993508e-03 1.17282541e-02\n",
      "  2.49865581e-03 2.07011844e-03 7.05848075e-03 2.91473512e-03\n",
      "  4.88415686e-03 1.28047923e-02 6.99153304e-01 2.61293352e-03\n",
      "  8.34920816e-03 5.37675396e-02 1.23784002e-02 3.41688137e-04\n",
      "  5.95094562e-02 3.36555801e-02 1.78567052e-03 7.00612692e-03\n",
      "  4.88643011e-04 9.53603128e-04]\n",
      " [7.87332356e-02 2.78927339e-03 1.10559594e-02 1.53919812e-02\n",
      "  8.45345203e-04 1.09377094e-02 1.54570118e-03 4.32916693e-02\n",
      "  2.94827740e-03 1.49475420e-02 2.30410211e-02 1.36158997e-02\n",
      "  2.97802389e-02 3.33660133e-02 2.13140085e-01 4.46079671e-02\n",
      "  3.97796417e-03 6.74488693e-02 8.85610934e-03 1.80016477e-02\n",
      "  2.54410096e-02 2.83361435e-01 2.12190859e-03 2.46217083e-02\n",
      "  2.50723343e-02 1.05906394e-03]\n",
      " [5.09147979e-02 3.65449442e-03 1.00083975e-02 5.72181381e-02\n",
      "  2.85762595e-04 3.86009575e-03 4.38201940e-03 8.98320898e-02\n",
      "  3.09805106e-03 4.72749732e-02 8.93657357e-02 3.03988196e-02\n",
      "  4.65324931e-02 5.61465546e-02 1.17215533e-02 4.36852127e-02\n",
      "  2.79779136e-02 1.89169329e-02 3.39777838e-03 1.21659338e-02\n",
      "  5.66396341e-02 5.17898016e-02 7.17330500e-02 1.48423299e-01\n",
      "  5.97162768e-02 8.60200904e-04]\n",
      " [1.17810639e-02 3.15350704e-02 3.94272394e-02 1.96867436e-03\n",
      "  2.81986147e-01 5.11278249e-02 5.57502545e-02 3.08600580e-03\n",
      "  1.23061857e-03 3.39769787e-04 9.87649243e-03 5.12806699e-03\n",
      "  7.05894025e-04 3.51662893e-04 1.14802115e-01 1.51733626e-02\n",
      "  2.26570992e-03 2.44001910e-01 1.18386589e-01 5.05823060e-04\n",
      "  1.85646943e-03 5.11111517e-04 4.98183617e-05 5.68376330e-04\n",
      "  4.54490328e-05 7.53852539e-03]\n",
      " [2.54453588e-02 8.33826198e-04 4.88261273e-03 9.86020360e-03\n",
      "  3.68923967e-04 3.87286441e-03 5.19934110e-04 1.30881695e-02\n",
      "  2.22393847e-03 2.13472638e-03 3.96823790e-03 1.51660806e-03\n",
      "  9.32582933e-03 3.76466922e-02 4.80666995e-01 1.68913358e-03\n",
      "  1.66163815e-03 4.50316221e-02 2.16111261e-03 1.08848768e-03\n",
      "  4.46588807e-02 2.99359411e-01 1.63262722e-03 5.00943838e-03\n",
      "  1.13782869e-03 2.14894899e-04]\n",
      " [4.81770262e-02 1.61402824e-03 3.16121103e-03 5.24257729e-03\n",
      "  2.98918050e-04 1.10170397e-03 6.46600558e-04 1.43865384e-02\n",
      "  4.77548776e-04 2.82137335e-04 6.45727059e-03 3.97325348e-04\n",
      "  6.91178208e-03 2.92973332e-02 6.67018712e-01 8.95348901e-04\n",
      "  2.75964988e-03 5.78130633e-02 2.29361164e-03 1.25504739e-04\n",
      "  4.13746908e-02 9.73209068e-02 2.10438529e-03 9.42587201e-03\n",
      "  2.79433356e-04 1.36918956e-04]\n",
      " [4.61765602e-02 1.58824341e-03 3.35580646e-03 6.32483559e-03\n",
      "  2.61404930e-04 1.05413178e-03 7.02711928e-04 1.49061708e-02\n",
      "  3.78522702e-04 2.32709004e-04 8.99108034e-03 4.00791672e-04\n",
      "  1.01001831e-02 3.68894860e-02 6.55460119e-01 7.71170715e-04\n",
      "  2.72911019e-03 5.35470396e-02 1.85664103e-03 9.33234478e-05\n",
      "  4.83824685e-02 8.88892710e-02 3.32965353e-03 1.32227447e-02\n",
      "  2.40856796e-04 1.14986695e-04]\n",
      " [2.45875139e-02 3.86028230e-04 4.86551132e-03 3.42785916e-03\n",
      "  3.24599969e-04 9.49315727e-03 3.26404464e-04 2.15018187e-02\n",
      "  3.18930927e-03 5.66905290e-02 8.74521770e-03 7.36340955e-02\n",
      "  8.65962822e-03 1.35976840e-02 1.28216594e-02 9.95311737e-02\n",
      "  6.53624360e-04 1.33935018e-02 2.07427540e-03 9.44971442e-02\n",
      "  5.47388336e-03 3.25670838e-01 5.89694537e-04 1.28412275e-02\n",
      "  2.02455893e-01 5.67680807e-04]\n",
      " [7.95577094e-02 2.80244183e-03 1.11049088e-02 1.60943419e-02\n",
      "  8.16579268e-04 1.01083070e-02 1.56447373e-03 4.33785096e-02\n",
      "  2.95128231e-03 1.46894520e-02 2.29133405e-02 1.35486024e-02\n",
      "  2.93990262e-02 3.48383226e-02 2.22184613e-01 4.00652401e-02\n",
      "  4.11759038e-03 6.56415150e-02 8.66960362e-03 1.59888975e-02\n",
      "  2.76900716e-02 2.79414892e-01 2.32528406e-03 2.50577983e-02\n",
      "  2.40294337e-02 1.04774185e-03]\n",
      " [1.41025875e-02 8.89943796e-04 5.51459240e-03 4.73906938e-03\n",
      "  3.45672597e-04 9.38359357e-04 4.42019111e-04 3.43490252e-03\n",
      "  7.28766317e-04 4.28205967e-04 1.90164999e-03 6.00627856e-04\n",
      "  1.97528256e-03 5.45039959e-03 8.81343663e-01 6.75800780e-04\n",
      "  1.16663729e-03 2.66354103e-02 3.36056878e-03 1.77624868e-04\n",
      "  1.63911525e-02 2.61464119e-02 2.59843655e-04 1.97418383e-03\n",
      "  1.21580757e-04 2.55009159e-04]]\n",
      "[INFO] O - 50.99%\n",
      "[INFO] E - 26.28%\n",
      "[INFO] O - 45.01%\n",
      "[INFO] O - 82.02%\n",
      "[INFO] O - 69.92%\n",
      "[INFO] V - 28.34%\n",
      "[INFO] X - 14.84%\n",
      "[INFO] E - 28.20%\n",
      "[INFO] O - 48.07%\n",
      "[INFO] O - 66.70%\n",
      "[INFO] O - 65.55%\n",
      "[INFO] V - 32.57%\n",
      "[INFO] V - 27.94%\n",
      "[INFO] O - 88.13%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.contours import sort_contours\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "model_path = r\"C:\\Projetos\\Mestrado\\Project II\\SourceCode\\TextIdentificationService\\model\\trained_ocr_only_az.model\"\n",
    "\n",
    "# load the handwriting OCR model\n",
    "print(\"[INFO] loading handwriting OCR model...\")\n",
    "model = load_model(model_path)\n",
    "print(model_path)\n",
    "\n",
    "# load the input image from disk, convert it to grayscale, and blur\n",
    "# it to reduce noise\n",
    "image_path = r\"C:\\Projetos\\Mestrado\\Project II\\SourceCode\\TextIdentificationService\\images\\texto.png\"\n",
    "image = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# perform edge detection, find contours in the edge map, and sort the\n",
    "# resulting contours from left-to-right\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
    "\n",
    "# initialize the list of contour bounding boxes and associated\n",
    "# characters that we'll be OCR'ing\n",
    "chars = []\n",
    "\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# compute the bounding box of the contour\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "\t# filter out bounding boxes, ensuring they are neither too small\n",
    "\t# nor too large\n",
    "\tif (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n",
    "\t\t# extract the character and threshold it to make the character\n",
    "\t\t# appear as *white* (foreground) on a *black* background, then\n",
    "\t\t# grab the width and height of the thresholded image\n",
    "\t\troi = gray[y:y + h, x:x + w]\n",
    "\t\tthresh = cv2.threshold(roi, 0, 255,\n",
    "\t\t\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\n",
    "\t\t# if the width is greater than the height, resize along the\n",
    "\t\t# width dimension\n",
    "\t\tif tW > tH:\n",
    "\t\t\tthresh = imutils.resize(thresh, width=32)\n",
    "\n",
    "\t\t# otherwise, resize along the height\n",
    "\t\telse:\n",
    "\t\t\tthresh = imutils.resize(thresh, height=32)\n",
    "\n",
    "\t\t# re-grab the image dimensions (now that its been resized)\n",
    "\t\t# and then determine how much we need to pad the width and\n",
    "\t\t# height such that our image will be 32x32\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\t\tdX = int(max(0, 32 - tW) / 2.0)\n",
    "\t\tdY = int(max(0, 32 - tH) / 2.0)\n",
    "\n",
    "\t\t# pad the image and force 32x32 dimensions\n",
    "\t\tpadded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n",
    "\t\t\tleft=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n",
    "\t\t\tvalue=(0, 0, 0))\n",
    "\t\tpadded = cv2.resize(padded, (32, 32))\n",
    "\n",
    "\t\t# prepare the padded image for classification via our\n",
    "\t\t# handwriting OCR model\n",
    "\t\tpadded = padded.astype(\"float32\") / 255.0\n",
    "\t\tpadded = np.expand_dims(padded, axis=-1)\n",
    "\n",
    "\t\t# update our list of characters that will be OCR'd\n",
    "\t\tchars.append((padded, (x, y, w, h)))\n",
    "\n",
    "# extract the bounding box locations and padded characters\n",
    "boxes = [b[1] for b in chars]\n",
    "chars = np.array([c[0] for c in chars], dtype=\"float32\")\n",
    "\n",
    "# OCR the characters using our handwriting recognition model\n",
    "preds = model.predict(chars)\n",
    "\n",
    "# define the list of label names\n",
    "#labelNames = \"0123456789\"\n",
    "labelNames = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "print(boxes)\n",
    "print(chars)\n",
    "print(preds)\n",
    "\n",
    "\n",
    "# loop over the predictions and bounding box locations together\n",
    "for (pred, (x, y, w, h)) in zip(preds, boxes):\n",
    "\t# find the index of the label with the largest corresponding\n",
    "\t# probability, then extract the probability and label\n",
    "\ti = np.argmax(pred)\n",
    "\tprob = pred[i]\n",
    "\tlabel = labelNames[i]\n",
    "\n",
    "\t# draw the prediction on the image\n",
    "\tprint(\"[INFO] {} - {:.2f}%\".format(label, prob * 100))\n",
    "\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\tcv2.putText(image, label, (x - 10, y - 10),\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "\n",
    "# show the image\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c24eadc24cde19d3a8e63a432684b30729b59b97047879d8d8cf2273aafd7845"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
